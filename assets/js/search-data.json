{
  
    
        "post0": {
            "title": "Solow Model",
            "content": "The Solow model is described by the following system of equations: . begin{align} Y_t &amp;= A_t F(K_t, L_t) &amp; &amp; mathbf{ text{Production Function}} Delta K_t &amp;= I_t - delta K_t &amp; &amp; mathbf{ text{Capital Accumulation}} S_t &amp;= s(Y_t - delta K_t) &amp; &amp; mathbf{ text{Net Saving Behaviour}} I_t - delta K_t &amp;= S_t &amp; &amp; mathbf{ text{Closed Economy}} end{align}Lets simplify it: . The Closed Economy and Net Saving Behaviour equations imply that net investment is equal to a fixed fraction $s$ of net income: $I_t - delta K_t = s(Y_t - delta K_t)$. We can then replace this into the Capital Accumulation equation to get: . $$ Delta K_t = s(Y_t - delta K_t)$$ . Lets stop to think about this equation for a moment. If there is some net income leftover after paying for restocking the capital that gets lost during the latest production ($Y_t - delta K_t &gt;0$), then we invest part of it into building new capital ($ Delta K_t &gt;0$). If, on the other hand, income is not enough to even pay for capital replacement, then capital falls next period ($ Delta K_t &lt;0$). . Finally, lets replace the production function into $Y_t$ to get the equation that summarizes all of the Solow model dynamics: . $$ Delta K_t = s(A_t F(K_t, L_t) - delta K_t) quad quad text{(In levels)}$$ . And we can rewrite it in growth rates when dividing both sides by $K_t$: . $$ frac{ Delta K_t}{K_t} = s frac{A_t F(K_t, L_t)}{K_t} - s delta quad quad text{(In growth rates)}$$ . Hence, so long as output per unit of capital is larger than what gets lost per unit of capital, capital will grow. . So the crucial part to understand capital growth is to figure out how output per unit of capital behaves. . Crucial Assumption . $$ frac{ partial F(K_t, L_t)}{ partial K_t} &gt; 0 quad frac{ partial^2 F(K_t, L_t)}{ partial K_t^2} &lt; 0 quad text{Decreasing returns to scale}$$ . With this assumption in place it means that as the economy grows, output per unit of capital $ frac{A_t F(K_t, L_t)}{K_t}$ falls. Which means that at some point it will be lower than depreciation $ delta$. . Steady State . There is a capital level in which total income matches the depreciation .",
            "url": "https://jbduarte.github.io/blog/economic%20growth/python/dash/2021/02/16/Solow.html",
            "relUrl": "/economic%20growth/python/dash/2021/02/16/Solow.html",
            "date": " • Feb 16, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "FAVAR - BBE(2005) Replication in R",
            "content": "The FAVAR model is represented by the following system: . $$ begin{align} X_t &amp;= Lambda^f F_t + Lambda^y Y_t + e_t begin{bmatrix} F_t Y_t end{bmatrix} &amp;= Psi(L) begin{bmatrix} F_{t-1} Y_{t-1} end{bmatrix} + v_t end{align} $$where $F_t$ is the collection of variables that is unobservable, while $Y_t$ is is of the observable. For further description of the FAVAR model and its properties please see BBE (QJE, 2005) . library(readxl) library(boot) library(tsDyn) library(vars) library(repr) . # Get original large dataset of BBE (2005): 120 series data = read_excel(&quot;./Data/bbe_data.xlsx&quot;) . head(data) . DateIPPIPFIPCIPCDIPCNIPEIPIIPMIPMD...PU85PUCPUCDPUSPUXFPUXHSPUXMLEHCCLEHMHHSNTN . 1959:01 | 0.013396992 | 0.008609752 | 0.007316259 | 0.005227654 | 0.009517142 | 0.013284200 | 0.018846549 | 0.031175802 | 0.045049989 | ... | 0.004728141 | 0.000000000 | 0.000000000 | 0.004357305 | 0.000000000 | 0.000000000 | 0.000000000 | 0.003478264 | 0.004618946 | 95.8 | . 1959:02 | 0.006022800 | 0.004916504 | 0.000000000 | 0.019405634 | -0.004747249 | 0.010731414 | 0.013885375 | 0.025638933 | 0.038650015 | ... | 0.004705891 | -0.003007521 | 0.005235614 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | -0.003478264 | 0.009174376 | 96.4 | . 1959:03 | 0.014342738 | 0.014547535 | 0.015653159 | 0.006396823 | 0.016505699 | 0.025817364 | 0.015959619 | 0.027254131 | 0.029650367 | ... | 0.000000000 | 0.000000000 | 0.000000000 | 0.004338402 | 0.003454235 | 0.000000000 | 0.000000000 | 0.006944472 | 0.004555816 | 96.9 | . 1959:04 | 0.008286750 | 0.009562755 | 0.004755444 | 0.020121006 | 0.000000000 | 0.031893735 | 0.005638694 | 0.025433102 | 0.034050126 | ... | 0.004683849 | 0.003007521 | 0.002607563 | 0.004319661 | 0.003442344 | 0.000000000 | 0.000000000 | 0.006896579 | 0.000000000 | 97.5 | . 1959:05 | 0.007036611 | 0.007141339 | -0.004755444 | 0.007458365 | -0.007033631 | 0.023250403 | 0.003368017 | -0.006587149 | -0.007362857 | ... | 0.004662013 | 0.000000000 | 0.000000000 | 0.000000000 | 0.003430535 | 0.003252035 | 0.003372685 | 0.003430535 | 0.004535155 | 97.2 | . 1959:06 | 0.001153906 | 0.008234277 | 0.013062217 | 0.019610667 | 0.008209100 | 0.002193842 | -0.013540671 | -0.061158362 | -0.105490860 | ... | 0.000000000 | 0.000000000 | 0.002600782 | 0.004301082 | 0.000000000 | 0.003241494 | 0.003361348 | 0.003418807 | 0.000000000 | 96.9 | . # Standardizing data = all variables with mean 0 and standard deviation 1. # This step is crucial in PC analysis data_s = scale(data[,2:121], center = TRUE, scale = TRUE) . Step 1: Extract principal componentes of all X (including Y) . pc_all = prcomp(data_s, center=FALSE, scale.=FALSE, rank. = 3) # summary(pc_all) C = pc_all$x # saving the principal components . Step 2: Extract principal componentes of Slow Variables . # Slow Variables slow_vars = c(&quot;IP&quot;, &quot;LHUR&quot;, &quot;PUNEW&quot;, &quot;IPP&quot;, &quot;IPF&quot;, &quot;IPC&quot;, &quot;IPCD&quot;, &quot;IPCN&quot;, &quot;IPE&quot;, &quot;IPI&quot;, &quot;IPM&quot;, &quot;IPMD&quot;, &quot;IPMND&quot;, &quot;IPMFG&quot;, &quot;IPD&quot;, &quot;IPN&quot;, &quot;IPMIN&quot;, &quot;IPUT&quot;, &quot;IPXMCA&quot;, &quot;PMI&quot;, &quot;PMP&quot;, &quot;GMPYQ&quot;, &quot;GMYXPQ&quot;, &quot;LHEL&quot;, &quot;LHELX&quot;, &quot;LHEM&quot;, &quot;LHNAG&quot;, &quot;LHU680&quot;, &quot;LHU5&quot;, &quot;LHU14&quot;, &quot;LHU15&quot;, &quot;LHU26&quot;, &quot;LPNAG&quot;, &quot;LP&quot;, &quot;LPGD&quot;, &quot;LPMI&quot;, &quot;LPCC&quot;, &quot;LPEM&quot;, &quot;LPED&quot;, &quot;LPEN&quot;, &quot;LPSP&quot;, &quot;LPTU&quot;, &quot;LPT&quot;, &quot;LPFR&quot;, &quot;LPS&quot;, &quot;LPGOV&quot;, &quot;LPHRM&quot;, &quot;LPMOSA&quot;, &quot;PMEMP&quot;, &quot;GMCQ&quot;, &quot;GMCDQ&quot;, &quot;GMCNQ&quot;, &quot;GMCSQ&quot;, &quot;GMCANQ&quot;, &quot;PWFSA&quot;, &quot;PWFCSA&quot;, &quot;PWIMSA&quot;, &quot;PWCMSA&quot;, &quot;PSM99Q&quot;, &quot;PU83&quot;, &quot;PU84&quot;, &quot;PU85&quot;, &quot;PUC&quot;, &quot;PUCD&quot;, &quot;PUS&quot;, &quot;PUXF&quot;, &quot;PUXHS&quot;, &quot;PUXM&quot;, &quot;LEHCC&quot;, &quot;LEHM&quot;) . data_slow = data_s[, slow_vars] pc_slow = prcomp(data_slow, center=FALSE, scale.=FALSE, rank. = 3) F_slow = pc_slow$x . Step 3: Clean the PC from the effect of observed Y . # Next clean the PC of all space from the observed Y reg = lm(C ~ F_slow + data_s[,&quot;FYFF&quot;]) #summary(reg) F_hat = C - data.matrix(data_s[,&quot;FYFF&quot;])%*%reg$coefficients[5,] # cleaning and saving F_hat . Step 4: Estimate FAVAR and get IRFs . . Warning: The IRFs in BBE are reported in standard deviation units, which means they are reported in the scaled data. No need to scale the data back to original units if want to compare with their paper. . data_var = data.frame(F_hat, &quot;FYFF&quot; = data_s[,&quot;FYFF&quot;]) var = VAR(data_var, p = 13) #summary(var) irf_point = irf(var, n.ahead = 48, impulse = &quot;FYFF&quot;, response = &quot;FYFF&quot;, boot = FALSE) # Shock size of 25 basis points impulse_sd = 0.25/sd(data$FYFF) scale = impulse_sd/(irf_point$irf$FYFF[1]) # position of FYFF response at step 0 # Computing Loading Factors reg_loadings = lm(data_s ~ 0 + F_hat + data_s[,&quot;FYFF&quot;]) loadings = reg_loadings$coefficients # head(reg_loadings$coefficients) #summary(reg_loadings) #### BOOTSTRAPING ######## R = 500 # Number of simulations nvars = 120 # Number of variables nsteps = 49 # numbers of steps IRFs = array(c(0,0,0), dim = c(nsteps,nvars,R)) var = lineVar(data_var, lag = 13, include = &quot;const&quot;) for(j in 1:R){ data_boot = VAR.boot(var, boot.scheme =&quot;resample&quot;) var_boot = VAR(data_boot, lag = 13) irf1 = irf(var_boot, n.ahead = 48, impulse = &quot;FYFF&quot;, boot = FALSE) for(i in 1:nvars){ IRFs[,i,j] = (irf1$irf$FYFF %*% matrix(loadings[1:4, i]))*scale } } ## Boot simulations done # Extract the quantiles of IRFs we are interested: 90% confidence intervals in BBE Upper = array(c(0,0), dim = c(nsteps, nvars)) for(k in 1:nsteps){ for(i in 1:nvars){ Upper[k,i] = quantile(IRFs[k,i,], probs = c(0.95))[1] } } Lower = array(c(0,0), dim = c(nsteps, nvars)) for(k in 1:nsteps){ for(i in 1:nvars){ Lower[k,i] = quantile(IRFs[k,i,], probs = c(0.05))[1] } } IRF = array(c(0,0), dim = c(nsteps, nvars)) for(k in 1:nsteps){ for(i in 1:nvars){ IRF[k,i] = quantile(IRFs[k,i,], probs = c(0.5))[1] } } rm(var_boot) rm(IRFs) . # Select the Variables you are Interested in # List of variables we are interested: FYFF, IP, CPI variables = c(grep(&quot;^FYFF$&quot;, colnames(data_s)), grep(&quot;^IP$&quot;, colnames(data_s)), grep(&quot;^PUNEW$&quot;, colnames(data_s)), grep(&quot;^FYGM3$&quot;, colnames(data_s)), grep(&quot;^FYGT5$&quot;, colnames(data_s)), grep(&quot;^FMFBA$&quot;, colnames(data_s)), grep(&quot;^FM2$&quot;, colnames(data_s)), grep(&quot;^EXRJAN$&quot;, colnames(data_s)), grep(&quot;^PMCP$&quot;, colnames(data_s)), grep(&quot;^IPXMCA$&quot;, colnames(data_s)), grep(&quot;^GMCQ$&quot;, colnames(data_s)), grep(&quot;^GMCDQ$&quot;, colnames(data_s)), grep(&quot;^GMCNQ$&quot;, colnames(data_s)), grep(&quot;^LHUR$&quot;, colnames(data_s)), grep(&quot;^PMEMP$&quot;, colnames(data_s)), grep(&quot;^LEHM$&quot;, colnames(data_s)), grep(&quot;^HSFR$&quot;, colnames(data_s)), grep(&quot;^PMNO$&quot;, colnames(data_s)), grep(&quot;^FSDXP$&quot;, colnames(data_s)), grep(&quot;^HHSNTN$&quot;, colnames(data_s)) ) transf_code = c(1, 5, 5, 1, 1, 5, 5, 5, 1, 1, 5, 5, 5, 1, 1, 5, 1, 1, 1, 1 ) variable_names = c(&quot;Fed Funds Rate&quot;, &quot;Industrial Production&quot;, &quot;CPI&quot;, &quot;3m Treasury Bills&quot;, &quot;5y Treasury Bonds&quot;, &quot;Monetary Base&quot;, &quot;M2&quot;, &quot;Exchange Rate Yen&quot;, &quot;Commodity Price Index&quot;, &quot;Capacity Util Rate&quot;, &quot;Personal Consumption&quot;, &quot;Durable Cons&quot;, &quot;Nondurable Cons&quot;, &quot;Unemployment&quot;, &quot;Employment&quot;, &quot;Avg Hourly Earnings&quot;, &quot;Housing Starts&quot;, &quot;New Orders&quot;, &quot;Dividends&quot;, &quot;Consumer Expectations&quot; ) . # Replicating Figure II in BBE (2005) - 3 Factors and Y = FYFF # Change plot size to 15 x 10 options(repr.plot.width=12, repr.plot.height=8) par(mfrow=c(5,4), mar = c(2, 2, 2, 2)) for(i in variables){ index = which(variables == i) if(transf_code[index] == 5){ plot(cumsum(IRF[,i]), type =&#39;l&#39;,lwd=2, main = variable_names[index], ylab= &quot;&quot;, xlab=&quot;Steps&quot;, ylim=range(cumsum(Lower[,i]),cumsum(Upper[,i])), cex.main=1.8, cex.axis=1.3) lines(cumsum(Upper[,i]), lty=2, col=&quot;red&quot;) lines(cumsum(Lower[,i]), lty=2, col=&quot;red&quot;) abline(h=0) } else{ plot(IRF[,i], type =&#39;l&#39;,lwd=2, main = variable_names[index], ylab= &quot;&quot;, xlab=&quot;Steps&quot;, ylim=range((Lower[,i]),(Upper[,i])), cex.main=1.8, cex.axis=1.3) lines((Upper[,i]), lty=2, col=&quot;red&quot;) lines((Lower[,i]), lty=2, col=&quot;red&quot;) abline(h=0) } } . How much of the variation is captured by the Factors and FYFF? . options(repr.plot.width=12, repr.plot.height=6) par(mfrow=c(1,2), mar = c(2, 2, 2, 2)) plot(data_s[, variables[2]], type = &quot;l&quot;, ylab = &quot;&quot;, main = &quot;Industrial Production&quot;) lines(fitted(reg_loadings)[,variables[2]], lty=2, col=&quot;red&quot;) legend(300,6, legend=c(&quot;Data&quot;, &quot;Predicted by PC&quot;), lty = 1:2, col = c(&quot;black&quot;, &quot;red&quot;), box.lty=0) plot(data_s[, variables[3]], type = &quot;l&quot;, ylab = &quot;&quot;, main = &quot;CPI&quot;) lines(fitted(reg_loadings)[,variables[3]], lty=2, col=&quot;red&quot;) . Step 5: FEVD . One step-ahead Variance Decomposition of $X_t$: . $$ E_t[X_{t+1} - X_t] = Lambda^f E_t[F_{t+1} - F_t] + Lambda^y E_t [Y_{t+1} - Y_t] + e_{t+1} $$$$ F_t = mu_f + L^1 varepsilon_t + A_1 L^1 varepsilon_t + ... $$$$ Y_t = mu_y + L^2 varepsilon_t + A_1 L^2 varepsilon_t + ... $$ where $L = begin{bmatrix} L^1 L^2 end{bmatrix}$ is the lower triangle matrix from the Cholesky decomposition. Let $ Lambda = begin{bmatrix} Lambda^f Lambda^y end{bmatrix}$, we have that: . $$ E_t[X_{t+1} - X_t] = Lambda L E_t[ varepsilon_{t+1}] + E_t[e_{t+1}] $$$$ E_t[X_{t+1} - X_t] = Psi(0) E_t[ varepsilon_{t+1}] + E_t[e_{t+1}] $$$$ var(E_t[X_{t+1} - X_t]) = Psi(0) Sigma_ varepsilon ( Psi(0)))&#39; + sigma^2_e $$where $ Sigma_ varepsilon = I $ because of the Cholesky decomposition, and $ Psi(0)$ is a 1x4 vector with the contemporaneous response of X_t to all structural shocks. Hence, the total variance of each observable variable $X_t$ is just the sum of the squared impulse response functions plus the variance of the measurement error. . At step-ahead $j$: . $$ var(E_t[X_{t+j} - X_t]) = sum_{i=0}^j Psi(i)( Psi(i)))&#39; + sigma^2_e $$ The variance of $E_t[X_{t+j} - X_t]$ that comes from the monetary policy shock alone is given by: . $$ var(E_t[X_{t+j} - X_t| varepsilon_F, e_t]) = sum_{i=0}^j Psi_{MP}(i)( Psi(i)_{MP}))&#39; $$where $ Psi(i)_{MP} = Psi(i)(1,4)$, i.e. the impulse response to monetary policy shock. Hence, the contribution of monetary policy shock to the $j$ step-ahead forecast error variance is given by: . $$ { text{FEVD}}(j)_{MP}^X = frac{var(E_t[X_{t+j} - X_t| varepsilon_F, e_t])}{var(E_t[X_{t+j} - X_t])} $$ # Get the VAR point estimates hor = 60 var = VAR(data_var, p = 13) irf_point = irf(var, n.ahead = hor, boot = FALSE) . # Get IRFs for all of X we are interested in, Dimensions: (hor, key_nvars) # Find loadings results = summary(reg_loadings) # the warning comes because of FYFF key_nvars = length(variables) irf_X_pc1 = array(c(0,0), dim=c(hor+1, key_nvars)) irf_X_pc2 = array(c(0,0), dim=c(hor+1, key_nvars)) irf_X_pc3 = array(c(0,0), dim=c(hor+1, key_nvars)) irf_X_fyff = array(c(0,0), dim=c(hor+1, key_nvars)) for(i in 1:key_nvars){ irf_X_pc1[,i] = irf_point$irf$PC1 %*% matrix(loadings[1:4, variables[i]]) irf_X_pc2[,i] = irf_point$irf$PC2 %*% matrix(loadings[1:4, variables[i]]) irf_X_pc3[,i] = irf_point$irf$PC3 %*% matrix(loadings[1:4, variables[i]]) irf_X_fyff[,i] = (irf_point$irf$FYFF) %*% matrix(loadings[1:4, variables[i]]) } . Warning message in summary.lm(object, ...): “essentially perfect fit: summary may be unreliable” . # Get the IRFs squared and accumulate them psi2_pc1 = array(0, dim = key_nvars) psi2_pc2 = array(0, dim = key_nvars) psi2_pc3 = array(0, dim = key_nvars) psi2_fyff = array(0, dim = key_nvars) for(i in 1:key_nvars){ for(j in 1:hor){ psi2_pc1[i] = psi2_pc1[i] + irf_X_pc1[j,i]^2 psi2_pc2[i] = psi2_pc2[i] + irf_X_pc2[j,i]^2 psi2_pc3[i] = psi2_pc3[i] + irf_X_pc3[j,i]^2 psi2_fyff[i] = psi2_fyff[i] + irf_X_fyff[j,i]^2 } } . var_total= array(0, dim = key_nvars) var_fac= array(0, dim = key_nvars) var_e= array(0, dim = key_nvars) for(i in 1:key_nvars){ var_fac[i] = psi2_pc1[i] + psi2_pc2[i] + psi2_pc3[i] + psi2_fyff[i] var_total[i] = psi2_pc1[i] + psi2_pc2[i] + psi2_pc3[i] + psi2_fyff[i] + results[[variables[i]]]$sigma^2 var_e[i] = results[[variables[i]]]$sigma^2 } . table = data.frame(&quot;PC1&quot; = round((psi2_pc1),3), &quot;PC2&quot; = round((psi2_pc2),3), &quot;PC3&quot; = round((psi2_pc3),3), &quot;FYFF&quot; = round((psi2_fyff),3), &quot;Factor_Y_total&quot; = round(var_fac,3) ,&quot;e&quot; = round((var_e),3), &quot;Total&quot; = round(var_total,3)) row.names(table) = variable_names table . PC1PC2PC3FYFFFactor_Y_totaleTotal . Fed Funds Rate0.497 | 0.279 | 0.059 | 0.150 | 0.985 | 0.000 | 0.985 | . Industrial Production0.550 | 0.037 | 0.054 | 0.147 | 0.787 | 0.255 | 1.042 | . CPI0.155 | 0.701 | 0.019 | 0.117 | 0.991 | 0.138 | 1.129 | . 3m Treasury Bills0.456 | 0.281 | 0.048 | 0.130 | 0.914 | 0.025 | 0.939 | . 5y Treasury Bonds0.299 | 0.309 | 0.015 | 0.087 | 0.710 | 0.068 | 0.778 | . Monetary Base0.017 | 0.056 | 0.010 | 0.005 | 0.087 | 0.900 | 0.988 | . M20.017 | 0.014 | 0.007 | 0.005 | 0.042 | 0.953 | 0.995 | . Exchange Rate Yen0.016 | 0.003 | 0.005 | 0.005 | 0.028 | 0.981 | 1.010 | . Commodity Price Index0.250 | 0.272 | 0.032 | 0.154 | 0.709 | 0.362 | 1.071 | . Capacity Util Rate0.365 | 0.069 | 0.057 | 0.146 | 0.637 | 0.252 | 0.889 | . Personal Consumption0.057 | 0.044 | 0.004 | 0.017 | 0.122 | 0.895 | 1.017 | . Durable Cons0.039 | 0.013 | 0.004 | 0.012 | 0.068 | 0.942 | 1.010 | . Nondurable Cons0.030 | 0.029 | 0.002 | 0.008 | 0.069 | 0.943 | 1.012 | . Unemployment0.251 | 0.176 | 0.064 | 0.088 | 0.578 | 0.182 | 0.760 | . Employment0.451 | 0.062 | 0.035 | 0.164 | 0.712 | 0.277 | 0.990 | . Avg Hourly Earnings0.040 | 0.160 | 0.007 | 0.026 | 0.232 | 0.792 | 1.025 | . Housing Starts0.280 | 0.033 | 0.023 | 0.093 | 0.428 | 0.600 | 1.028 | . New Orders0.437 | 0.055 | 0.044 | 0.156 | 0.691 | 0.350 | 1.041 | . Dividends0.109 | 0.346 | 0.007 | 0.040 | 0.502 | 0.448 | 0.950 | . Consumer Expectations0.173 | 0.462 | 0.014 | 0.061 | 0.710 | 0.290 | 1.000 | . # Replicating Table I in BBE (2005) - 3 Factors and Y = FYFF r2 = array(0, dim = key_nvars) for(i in 1:key_nvars){ r2[i] = results[[variables[i]]]$r.squared } . table2 = data.frame(&quot;Variables&quot; = variable_names, &quot;Contribution&quot; = round((psi2_fyff/var_total),3), &quot;R-squared&quot; = round(r2,3)) table2 . VariablesContributionR.squared . Fed Funds Rate | 0.152 | 1.000 | . Industrial Production | 0.141 | 0.746 | . CPI | 0.104 | 0.863 | . 3m Treasury Bills | 0.139 | 0.975 | . 5y Treasury Bonds | 0.112 | 0.933 | . Monetary Base | 0.005 | 0.105 | . M2 | 0.005 | 0.052 | . Exchange Rate Yen | 0.005 | 0.024 | . Commodity Price Index | 0.144 | 0.640 | . Capacity Util Rate | 0.165 | 0.749 | . Personal Consumption | 0.017 | 0.110 | . Durable Cons | 0.012 | 0.063 | . Nondurable Cons | 0.008 | 0.063 | . Unemployment | 0.116 | 0.819 | . Employment | 0.166 | 0.724 | . Avg Hourly Earnings | 0.025 | 0.212 | . Housing Starts | 0.090 | 0.404 | . New Orders | 0.150 | 0.652 | . Dividends | 0.042 | 0.555 | . Consumer Expectations | 0.061 | 0.712 | .",
            "url": "https://jbduarte.github.io/blog/time%20series/r/favar/2020/04/24/FAVAR-Replication.html",
            "relUrl": "/time%20series/r/favar/2020/04/24/FAVAR-Replication.html",
            "date": " • Apr 24, 2020"
        }
        
    
  

  
  

  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jbduarte.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}